{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fafe9e99",
      "metadata": {
        "id": "fafe9e99"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs-stat-180/blob/main/notebooks/05-data-preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfea1ba",
      "metadata": {
        "id": "edfea1ba"
      },
      "source": [
        "\n",
        "# From Mess to Model: Mastering Data Preparation\n",
        "\n",
        "This notebook accompanies the lecture and provides runnable examples covering:\n",
        "- Data discovery & profiling  \n",
        "- Cleaning (missing values, inconsistencies, duplicates, outliers)  \n",
        "- Transformation (scaling, aggregation)  \n",
        "- Feature engineering  \n",
        "- A mini case study for customer churn preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NTTWjYfQrbQi",
      "metadata": {
        "id": "NTTWjYfQrbQi"
      },
      "source": [
        "## Setup: Loading Files into Colab environment\n",
        "This method works in any Python environment (like a local Jupyter Notebook or a script) and doesn't require installing special libraries, but it does require you to change the sharing settings of your file.\n",
        "\n",
        "* **Step 1**: Share the File in Google Drive\n",
        "Go to your Google Drive, right-click on worldcities.csv, and select Share. In the sharing settings, change the access from \"Restricted\" to \"Anyone with the link\".\n",
        "\n",
        "* **Step 2**: Copy the Link and Extract the File ID\n",
        "Copy the shareable link. It will look like this:\n",
        "https://drive.google.com/file/d/SOME_LONG_FILE_ID/view?usp=sharing\n",
        "\n",
        "The important part is the FILE_ID, which is the long string of characters between /d/ and /view.\n",
        "\n",
        "* **Step 3**: Construct the Direct Download URL and Read with Pandas\n",
        "You can use the FILE_ID to create a direct download link. The following Python code does this for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d30aaa",
      "metadata": {
        "id": "f4d30aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (12, 10)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Age</th>\n",
              "      <th>State</th>\n",
              "      <th>JoinDate</th>\n",
              "      <th>LastPurchaseDate</th>\n",
              "      <th>TotalSpend</th>\n",
              "      <th>Income</th>\n",
              "      <th>Churn</th>\n",
              "      <th>Address</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>101</td>\n",
              "      <td>Jane Doe</td>\n",
              "      <td>29.0</td>\n",
              "      <td>CA</td>\n",
              "      <td>2020-01-15</td>\n",
              "      <td>2023-11-01</td>\n",
              "      <td>1200</td>\n",
              "      <td>55000.0</td>\n",
              "      <td>0</td>\n",
              "      <td>123 Maple St, San Francisco, CA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102</td>\n",
              "      <td>John Smith</td>\n",
              "      <td>NaN</td>\n",
              "      <td>California</td>\n",
              "      <td>2019-06-20</td>\n",
              "      <td>2024-02-10</td>\n",
              "      <td>3400</td>\n",
              "      <td>72000.0</td>\n",
              "      <td>1</td>\n",
              "      <td>77 Elm Road, Los Angeles, California</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>103</td>\n",
              "      <td>Ana Lopez</td>\n",
              "      <td>41.0</td>\n",
              "      <td>TX</td>\n",
              "      <td>2018-03-10</td>\n",
              "      <td>2022-12-15</td>\n",
              "      <td>5600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9 1st Ave, Austin, TX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104</td>\n",
              "      <td>Wei Chen</td>\n",
              "      <td>35.0</td>\n",
              "      <td>NY</td>\n",
              "      <td>2021-07-05</td>\n",
              "      <td>2023-07-30</td>\n",
              "      <td>800</td>\n",
              "      <td>49000.0</td>\n",
              "      <td>0</td>\n",
              "      <td>456 Pine St, New York, NY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105</td>\n",
              "      <td>Maria Rossi</td>\n",
              "      <td>28.0</td>\n",
              "      <td>Calif.</td>\n",
              "      <td>2020-11-12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2000</td>\n",
              "      <td>61000.0</td>\n",
              "      <td>1</td>\n",
              "      <td>88 Oak Blvd, San Diego, Calif.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CustomerID         Name   Age       State    JoinDate LastPurchaseDate  TotalSpend   Income  Churn  \\\n",
              "0         101     Jane Doe  29.0          CA  2020-01-15       2023-11-01        1200  55000.0      0   \n",
              "1         102   John Smith   NaN  California  2019-06-20       2024-02-10        3400  72000.0      1   \n",
              "2         103    Ana Lopez  41.0          TX  2018-03-10       2022-12-15        5600      NaN      0   \n",
              "3         104     Wei Chen  35.0          NY  2021-07-05       2023-07-30         800  49000.0      0   \n",
              "4         105  Maria Rossi  28.0      Calif.  2020-11-12              NaN        2000  61000.0      1   \n",
              "\n",
              "                                Address  \n",
              "0       123 Maple St, San Francisco, CA  \n",
              "1  77 Elm Road, Los Angeles, California  \n",
              "2                 9 1st Ave, Austin, TX  \n",
              "3             456 Pine St, New York, NY  \n",
              "4        88 Oak Blvd, San Diego, Calif.  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 120)\n",
        "\n",
        "# Paste the FILE_ID you extracted from the shareable link\n",
        "file_id = '1-qH2pQIP9LNe3XO6mxLH9wlbbfTlT-8W'\n",
        "\n",
        "# This creates the direct download URL\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first 5 rows to confirm it loaded correctly\n",
        "\n",
        "print(\"Shape:\",df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74614111",
      "metadata": {
        "id": "74614111"
      },
      "source": [
        "\n",
        "## Introduction: Why Data Preparation?\n",
        "\n",
        "> *Garbage in, garbage out.* Models cannot fix bad data.  \n",
        "Data preparation typically consumes a large proportion of project time and directly impacts model accuracy and reliability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb8c5b4",
      "metadata": {
        "id": "5fb8c5b4"
      },
      "source": [
        "## Step 1: Data Profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4ace4c",
      "metadata": {
        "id": "5a4ace4c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inspect structure, types, and missingness\n",
        "df.info()\n",
        "display(df.isna().sum())\n",
        "df.describe()\n",
        "\n",
        "# Random sample 5 of rows\n",
        "df.sample(5, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aa0616",
      "metadata": {
        "id": "11aa0616"
      },
      "source": [
        "## Step 2: Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j26npEkbwrfW",
      "metadata": {
        "id": "j26npEkbwrfW"
      },
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d915820",
      "metadata": {
        "id": "4d915820"
      },
      "outputs": [],
      "source": [
        "# Example strategies:\n",
        "# 1) Deletion (listwise) — only when safe and small proportion missing\n",
        "df_del = df.dropna(subset=['Age', 'Income'], how='any')\n",
        "print(\"Listwise deletion shape:\", df_del.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U747aND6S5w5",
      "metadata": {
        "id": "U747aND6S5w5"
      },
      "outputs": [],
      "source": [
        "# 2) Deletion (column-wise) - when a large proportion is missing\n",
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = df.isnull().sum() / len(df) * 100\n",
        "\n",
        "# Identify columns with missing percentage greater than 10%\n",
        "cols_to_drop = missing_percentage[missing_percentage > 10].index\n",
        "\n",
        "# Drop these columns from the DataFrame\n",
        "df_col_dropped = df.drop(columns=cols_to_drop)\n",
        "\n",
        "print(\"Original shape:\", df.shape)\n",
        "print(\"Shape after dropping columns with >10% missing values:\", df_col_dropped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C3NaAoo-To6C",
      "metadata": {
        "id": "C3NaAoo-To6C"
      },
      "outputs": [],
      "source": [
        "# 3) Deletion (listwise) - only when safe and small proportion missing\n",
        "df_del_any = df.dropna(how='any')\n",
        "print(\"Listwise deletion (any NaN) shape:\", df_del_any.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BEFUXlwvbCJ",
      "metadata": {
        "id": "-BEFUXlwvbCJ"
      },
      "outputs": [],
      "source": [
        "# 4) Simple imputation\n",
        "df_imp = df.copy()\n",
        "df_imp['Age'] = df_imp['Age'].fillna(df_imp['Age'].median())\n",
        "df_imp['Income'] = df_imp['Income'].fillna(df_imp['Income'].median())\n",
        "df_imp[['Age','Income']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TVs9VAX4veeb",
      "metadata": {
        "id": "TVs9VAX4veeb"
      },
      "outputs": [],
      "source": [
        "# 5) Predictive imputation can be done with models (skipped for brevity here)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfeb7690",
      "metadata": {
        "id": "cfeb7690"
      },
      "source": [
        "### Inaccurate & Inconsistent Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uTE1fbiny4DZ",
      "metadata": {
        "id": "uTE1fbiny4DZ"
      },
      "outputs": [],
      "source": [
        "df_imp['State'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85339a26",
      "metadata": {
        "id": "85339a26"
      },
      "outputs": [],
      "source": [
        "# Standardize State column (CA, California, Calif. -> CA)\n",
        "state_map = {'California': 'CA', 'Calif.': 'CA'}\n",
        "df_imp['State'] = df_imp['State'].replace(state_map)\n",
        "df_imp['State'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rM10ZWFTx95V",
      "metadata": {
        "id": "rM10ZWFTx95V"
      },
      "outputs": [],
      "source": [
        "display(df_imp[['JoinDate','LastPurchaseDate']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y_ysyMfKwpg2",
      "metadata": {
        "id": "y_ysyMfKwpg2"
      },
      "outputs": [],
      "source": [
        "# Ensure dates are parsed and consistent\n",
        "\n",
        "df_imp['JoinDate'] = pd.to_datetime(df_imp['JoinDate'], errors='coerce')\n",
        "df_imp['LastPurchaseDate'] = pd.to_datetime(df_imp['LastPurchaseDate'], errors='coerce')\n",
        "df_imp.info()\n",
        "df_imp[['JoinDate','LastPurchaseDate']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g3K3duBvxYUo",
      "metadata": {
        "id": "g3K3duBvxYUo"
      },
      "outputs": [],
      "source": [
        "df_imp['Name'].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hgj0KY8GwsNj",
      "metadata": {
        "id": "hgj0KY8GwsNj"
      },
      "outputs": [],
      "source": [
        "# Trim/canonicalize text fields (example: Name)\n",
        "df_imp['Name'] = df_imp['Name'].str.strip()\n",
        "\n",
        "df_imp[['State','JoinDate','LastPurchaseDate','Name']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005df2c0",
      "metadata": {
        "id": "005df2c0"
      },
      "source": [
        "### Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b320d810",
      "metadata": {
        "id": "b320d810"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Count duplicates (all columns)\n",
        "dup_count = df_imp.duplicated().sum()\n",
        "print(\"Exact duplicate rows:\", dup_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FQWvEM8ixobZ",
      "metadata": {
        "id": "FQWvEM8ixobZ"
      },
      "outputs": [],
      "source": [
        "# Identify potential dupes by a key subset (e.g., Name + JoinDate + State)\n",
        "subset_dupes = df_imp.duplicated(subset=['Name','JoinDate','State']).sum()\n",
        "print(\"Subset-based duplicate rows:\", subset_dupes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5F_HyY1Rxqpo",
      "metadata": {
        "id": "5F_HyY1Rxqpo"
      },
      "outputs": [],
      "source": [
        "# Drop exact duplicates\n",
        "df_nodup = df_imp.drop_duplicates()\n",
        "df_nodup.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759a1397",
      "metadata": {
        "id": "759a1397"
      },
      "source": [
        "## Step 3: Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a3c7e0",
      "metadata": {
        "id": "43a3c7e0"
      },
      "source": [
        "### Normalization (Min-Max) vs Standardization (Z-score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9563cbc",
      "metadata": {
        "id": "e9563cbc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "scale_df = df_capped[['Income','TotalSpend_capped']].dropna().copy()\n",
        "\n",
        "# Min-Max Scaling\n",
        "mm = MinMaxScaler()\n",
        "scale_df['Income_MinMax'] = mm.fit_transform(scale_df[['Income']])\n",
        "scale_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wtf0VsXbyE68",
      "metadata": {
        "id": "Wtf0VsXbyE68"
      },
      "outputs": [],
      "source": [
        "# Standardization\n",
        "ss = StandardScaler()\n",
        "scale_df['Income_Standard'] = ss.fit_transform(scale_df[['Income']])\n",
        "scale_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014b0e15",
      "metadata": {
        "id": "014b0e15"
      },
      "source": [
        "### Data Aggregation (monthly sales example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8183693c",
      "metadata": {
        "id": "8183693c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Aggregate by month using LastPurchaseDate and TotalSpend\n",
        "agg = df_capped.dropna(subset=['LastPurchaseDate']).copy()\n",
        "agg['Month'] = agg['LastPurchaseDate'].dt.to_period('M')\n",
        "monthly_sales = agg.groupby('Month', as_index=False)['TotalSpend'].sum()\n",
        "\n",
        "print(\"Monthly sales (sum of TotalSpend by last purchase month):\")\n",
        "display(monthly_sales)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y230HdEkySwE",
      "metadata": {
        "id": "Y230HdEkySwE"
      },
      "outputs": [],
      "source": [
        "# Simple plot\n",
        "plt.figure()\n",
        "plt.plot(monthly_sales['Month'].astype(str), monthly_sales['TotalSpend'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Monthly Sales')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('TotalSpend')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10211b9f",
      "metadata": {
        "id": "10211b9f"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a227f51",
      "metadata": {
        "id": "8a227f51"
      },
      "outputs": [],
      "source": [
        "fe = df_capped.copy()\n",
        "\n",
        "# Day of week from LastPurchaseDate\n",
        "fe['DayOfWeek'] = fe['LastPurchaseDate'].dt.day_name()\n",
        "\n",
        "# Days since last purchase (relative to \"today\")\n",
        "today = pd.Timestamp.today().normalize()\n",
        "fe['DaysSinceLastPurchase'] = (today - fe['LastPurchaseDate']).dt.days\n",
        "\n",
        "# Simple CLV proxy: average monthly spend over recency (protect against div by zero)\n",
        "fe['MonthsSinceLastPurchase'] = np.maximum(fe['DaysSinceLastPurchase'] / 30.0, 1.0)\n",
        "fe['CLV_proxy'] = fe['TotalSpend'] / fe['MonthsSinceLastPurchase']\n",
        "\n",
        "fe[['CustomerID','Name','DayOfWeek','DaysSinceLastPurchase','CLV_proxy']].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f209557",
      "metadata": {
        "id": "2f209557"
      },
      "source": [
        "## Mini Case Study: Preparing Customer Data for Churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ae78d9",
      "metadata": {
        "id": "21ae78d9"
      },
      "outputs": [],
      "source": [
        "# Start from raw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "raw = pd.read_csv(url)\n",
        "\n",
        "# Profile the data\n",
        "raw.head()\n",
        "raw.info()\n",
        "raw.describe()\n",
        "raw.isnull().sum()\n",
        "\n",
        "# 1) Make categorical names consistent and convert dates (stings) to datetime\n",
        "raw['State'] = raw['State'].replace({'California':'CA','Calif.':'CA'})\n",
        "raw['JoinDate'] = pd.to_datetime(raw['JoinDate'], errors='coerce')\n",
        "raw['LastPurchaseDate'] = pd.to_datetime(raw['LastPurchaseDate'], errors='coerce')\n",
        "\n",
        "# 2) Impute numeric columns (median)\n",
        "for col in ['Age','Income']:\n",
        "    raw[col] = raw[col].fillna(raw[col].median())\n",
        "\n",
        "# 3) Handle missing LastPurchaseDate: fill with JoinDate as fallback (domain choice for demo)\n",
        "raw['LastPurchaseDate'] = raw['LastPurchaseDate'].fillna(raw['JoinDate'])\n",
        "\n",
        "# 4) Remove duplicates\n",
        "clean = raw.drop_duplicates()\n",
        "\n",
        "# 5) Feature engineering\n",
        "today = pd.Timestamp.today().normalize()\n",
        "clean['DaysSinceLastPurchase'] = (today - clean['LastPurchaseDate']).dt.days\n",
        "clean['TenureDays'] = (today - clean['JoinDate']).dt.days\n",
        "clean['AvgMonthlySpend'] = clean['TotalSpend'] / np.maximum(clean['TenureDays']/30.0, 1.0)\n",
        "\n",
        "# Simple CLV proxy: AvgMonthlySpend * 6 months horizon\n",
        "clean['CustomerLifetimeValue_6mo'] = clean['AvgMonthlySpend'] * 6\n",
        "\n",
        "# 6) Scaling selected features (for model input)\n",
        "for_scale = clean[['Income','TotalSpend','DaysSinceLastPurchase','TenureDays','CustomerLifetimeValue_6mo']].copy()\n",
        "scaler = StandardScaler()\n",
        "scaled = pd.DataFrame(scaler.fit_transform(for_scale), columns=[c + \"_z\" for c in for_scale.columns])\n",
        "\n",
        "model_input = pd.concat([clean[['CustomerID','Churn','State']], scaled], axis=1)\n",
        "model_input.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf12cd2",
      "metadata": {
        "id": "8bf12cd2"
      },
      "source": [
        "\n",
        "## Conclusion & Key Takeaways\n",
        "\n",
        "- Data preparation is iterative: profile → clean → transform → engineer → (repeat as needed).  \n",
        "- Document your assumptions and choices (e.g., why you imputed a value a certain way).  \n",
        "- High-quality preparation underpins trustworthy, impactful models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uSkC1B2sqif2",
        "NTTWjYfQrbQi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
